<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>!!Check to see if it is embedding every time – Dungeons &amp; Dragons AI Assistant</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-ff4892edf4a7050d1342fad5826b0fcc.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Dungeons &amp; Dragons AI Assistant</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text"><i class="bi bi-house"></i> Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../D&amp;D Chat Bot.html"> 
<span class="menu-text"><i class="bi bi-newspaper"></i> D&amp;D Chat Bot</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../Roadmap.html"> 
<span class="menu-text"><i class="bi bi-pencil"></i> Roadmap</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">!!Check to see if it is embedding every time</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div id="e23a8bfb" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> load_documents <span class="im">import</span> load_docs</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>docs <span class="op">=</span> load_docs()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="302531c3" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total docs embedded: </span><span class="sc">{</span><span class="bu">len</span>(docs)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total docs embedded: 329</code></pre>
</div>
</div>
<div id="cec1303f" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>docs[:<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>['type: spell\nname: Aid\ndescription: Your spell bolsters your allies with toughness and \nresolve. C hoose up to three creatures within range.\nEach target’s hit point maximum and current hit points \nincrease by 5 for the duration.\nAt Higher Levels. W hen you cast this spell using \na spell slot of 3rd level or higher, a target’s hit points \nincrease by an additional 5 for each slot level above 2nd.\nschool: abjuration\nlevel: 2nd-level\ncasting_time: 1 action\nrange: 30 feet\ncomponents: V, S, M (a tiny strip o f white cloth)\nduration: 8 hours']</code></pre>
</div>
</div>
<div id="fa3ef884" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>docs <span class="op">=</span> load_docs()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total docs: </span><span class="sc">{</span><span class="bu">len</span>(docs)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Examples:"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, doc <span class="kw">in</span> <span class="bu">enumerate</span>(docs[:<span class="dv">3</span>]):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- doc[</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">] ---</span><span class="ch">\n</span><span class="sc">{</span>doc<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total docs: 329
Examples:

--- doc[0] ---
type: spell
name: Aid
description: Your spell bolsters your allies with toughness and 
resolve. C hoose up to three creatures within range.
Each target’s hit point maximum and current hit points 
increase by 5 for the duration.
At Higher Levels. W hen you cast this spell using 
a spell slot of 3rd level or higher, a target’s hit points 
increase by an additional 5 for each slot level above 2nd.
school: abjuration
level: 2nd-level
casting_time: 1 action
range: 30 feet
components: V, S, M (a tiny strip o f white cloth)
duration: 8 hours

--- doc[1] ---
type: spell
name: Alarm
description: You set an alarm against unwanted intrusion. Choose 
a door, a window, or an area within range that is no 
larger than a 20-foot cube. Until the spell ends, an alarm 
alerts you whenever a Tiny or larger creature touches 
or enters the warded area. W hen you cast the spell, you 
can designate creatures that w on’t set off the alarm. You 
also choose whether the alarm is mental or audible.
A mental alarm alerts you with a ping in your mind 
if you are within 1 mile of the warded area. This ping 
awakens you if you are sleeping.
An audible alarm produces the sound of a hand bell 
for 10 seconds within 60 feet.
school: abjuration
level: 1st-level
casting_time: 1 minute
range: 30 feet
components: V, S, M (a tiny bell and a piece of
duration: 8 hours

--- doc[2] ---
type: spell
name: AlterSelf
description: You assume a different form. W hen you cast the spell, 
choose one of the following options, the effects of which 
last for the duration of the spell. W hile the spell lasts,
school: transmutation
level: 2nd-level
casting_time: 1 action
range: Self
components: V, S
duration: Concentration, up to 1 hour</code></pre>
</div>
</div>
<div id="b5dd1e92" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Spells:"</span>, <span class="bu">len</span>(docs[<span class="dv">0</span>]))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Weapons:"</span>, <span class="bu">len</span>(docs[<span class="dv">1</span>]))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Feats:"</span>, <span class="bu">len</span>(docs[<span class="dv">2</span>]))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classes:"</span>, <span class="bu">len</span>(docs[<span class="dv">3</span>]))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(docs))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Spells: 540
Weapons: 792
Feats: 344
Classes: 716
329</code></pre>
</div>
</div>
<div id="223888a2" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f639ee47" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install sentence-transformers faiss-cpu gradio</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">#INSTALL ME</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1708953c" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> faiss</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2195017d" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model = "sentence-transformers/all-MiniLM-L6-v2"</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># model = 'bert-base-nli-mean-tokens'</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span><span class="st">'all-mpnet-base-v2'</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>embedder <span class="op">=</span> SentenceTransformer(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="969623b1" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>question_cache <span class="op">=</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5caf1150" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Example  and Demonstration Code</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print(embedder.encode('Hello!'))</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> [<span class="st">'Hello how are you today?'</span>, <span class="st">'Hi how was yoru day?'</span>]</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>example <span class="op">=</span>embedder.encode(sentence).shape</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(embedder.encode(sentence).shape)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer, util</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>embedder <span class="op">=</span> SentenceTransformer(<span class="st">'all-MiniLM-L6-v2'</span>)  <span class="co"># or your chosen model</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> [<span class="st">'Hello how are you today?'</span>, <span class="st">'Hi how was your day?'</span>]</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> embedder.encode(sentence, convert_to_tensor<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>similarities <span class="op">=</span> util.cos_sim(embeddings, embeddings)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(similarities)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(2, 768)
tensor([[1.0000, 0.8333],
        [0.8333, 1.0000]])</code></pre>
</div>
</div>
<div id="c388d934" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>embs <span class="op">=</span> embedder.encode(docs, batch_size<span class="op">=</span><span class="dv">64</span>, convert_to_numpy<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">#pass in the shape of youre embeedding</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> faiss.IndexFlatL2(embs.shape[<span class="dv">1</span>])</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(index.is_trained)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>index.add(embs)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Number of vectors in our index</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(index.ntotal)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True
329</code></pre>
</div>
</div>
<div id="dabb28df" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>time</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">#embedd this sentenace </span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>xq <span class="op">=</span> embedder.encode([<span class="st">'can you tell me the damage a longsword does?'</span>])</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">#k is the number of similar verctors you would like to return </span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>k<span class="op">=</span><span class="dv">1</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># In our index, seach for a vector that is similar to xq. And returns the Vector IDs 162</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>D,I <span class="op">=</span> index.search(xq,k)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(I)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Returns the vectors that are most simialr from docs</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> I[<span class="dv">0</span>]:</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(docs[i])</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co">#  # or your chosen model</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co"># sentence = ['can you tell me the damage a longsword does?', 'type: spell name: MeteorSwarm description: Blazing orbs of fire plummet to the ground at four different points you can see within range. Each creature ']</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> embedder.encode(sentence, convert_to_tensor<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>similarities <span class="op">=</span> util.cos_sim(embeddings, embeddings)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(similarities)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[162]]


type: spell
name: MeteorSwarm
description: Blazing orbs of fire plummet to the ground at four 
different points you can see within range. Each creature 
in a 40-foot-radius sphere centered on each point you 
choose must make a Dexterity saving throw. The sphere 
spreads around corners. A creature takes 20d6 fire 
damage and 20d6 bludgeoning damage on a failed 
save, or half as much damage on a successful one. A 
creature in the area of m ore than one fiery burst is 
affected only once.
The spell dam ages objects in the area and ignites 
flammable objects that aren’t being worn or carried.
M i n d  B l a n k
8 th-level abjuration
Casting Time: 1 action 
Range: Touch 
Components: V, S 
Duration: 24 hours
Until the spell ends, one willing creature you touch is 
immune to psychic damage, any effect that would sense
school: evocation
level: 9th-level
casting_time: 1 action
range: 1 mile
components: V, S
duration: Instantaneous


tensor([[1.0000, 0.8333],
        [0.8333, 1.0000]])
CPU times: user 68.2 ms, sys: 7.92 ms, total: 76.1 ms
Wall time: 12.5 ms</code></pre>
</div>
</div>
<div id="dccc0ccb" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> retrieve(question: <span class="bu">str</span>, k: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    q_emb <span class="op">=</span> embedder.encode([question]).astype(<span class="st">"float32"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    D, I <span class="op">=</span> index.search(q_emb, k)                <span class="co"># distances &amp; indices</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    hits <span class="op">=</span> [docs[i] <span class="cf">for</span> i <span class="kw">in</span> I[<span class="dv">0</span>]]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>.join(hits)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0ce32950" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline, AutoTokenizer</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>   <span class="co"># any small instruct model works</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>tok  <span class="op">=</span> AutoTokenizer.from_pretrained(model_name, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>llm  <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                model<span class="op">=</span>model_name,</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                tokenizer<span class="op">=</span>tok,</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                device<span class="op">=</span><span class="st">"cpu"</span>,           </span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>                max_new_tokens<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>                temperature<span class="op">=</span><span class="fl">.01</span>)        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use cpu</code></pre>
</div>
</div>
<section id="is-this-cache-causing-a-problem-by-taking-up-too-much-memory-is-there-way-to-store-previous-questions-in-a-database-or-file-outside-of-memory" class="level4">
<h4 class="anchored" data-anchor-id="is-this-cache-causing-a-problem-by-taking-up-too-much-memory-is-there-way-to-store-previous-questions-in-a-database-or-file-outside-of-memory">is this cache causing a problem by taking up too much memory? is there way to store previous questions in a database or file outside of memory?</h4>
<div id="fbeccdc5" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers.util <span class="im">import</span> cos_sim</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_cached_answer(query, threshold<span class="op">=</span><span class="fl">0.90</span>):</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    q_emb <span class="op">=</span> embedder.encode(query, convert_to_tensor<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> question_cache:</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        sim <span class="op">=</span> cos_sim(q_emb, item[<span class="st">'embedding'</span>]).item()</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sim <span class="op">&gt;=</span> threshold:</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"⚡ Using cached answer (similarity=</span><span class="sc">{</span>sim<span class="sc">:.2f}</span><span class="ss">) for: </span><span class="sc">{</span>item[<span class="st">'question'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> item[<span class="st">'answer'</span>]</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># elif sim &lt;= .6:</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">#     return print('Please reference the rulebook')</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> answer(question: <span class="bu">str</span>):</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    cached <span class="op">=</span> get_cached_answer(question)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cached:</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cached</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> retrieve(question, k<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    prompt <span class="op">=</span> <span class="ss">f"""You are a helpful assistant. </span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a><span class="ss">    Answer the question using only the context below. </span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="ss">    If the answer is not in the context, say you don't know. </span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a><span class="ss">    Ensure the answers don't have duplicate information.</span></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="ss">    When providing an answer:</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a><span class="ss">    - Ensure clarity and conciseness.</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a><span class="ss">    - If listing items (e.g., spells, weapons, races, features), return only **unique** items. Avoid duplicates or synonyms.</span></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="ss">    - Format your answer as a **numbered list** or **clear bullet points** if appropriate.</span></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a><span class="ss">    - Never invent facts outside the provided context.</span></span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a><span class="ss">    You are a Dungeon Master guiding players through a high-fantasy tabletop role-playing game. You have access to private source data including maps, NPC backstories, world lore, secret quest logic, and random outcome rules. You use this source data to maintain a consistent, immersive world and adapt to player decisions.</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a><span class="ss">You must respond in **structured JSON format** with the following fields:</span></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a><span class="ss">  "narration": "A vivid, immersive description of what the player experiences based on their action or question.",</span></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a><span class="ss">  "player_options": "A list of clear, relevant actions the player might consider next.",</span></span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a><span class="ss">  "hidden_logic": "Any behind-the-scenes interpretation, dice outcomes, or consequences that should NOT be shown to the player.",</span></span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a><span class="ss">  "dm_notes": "Optional notes for the Dungeon Master (not shown to players) that track state, foreshadow, or suggest future branches."</span></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a><span class="ss">Guidelines:</span></span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- Use rich sensory language in the `narration` to describe environments and NPCs.</span></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- Present `player_options` as concise, relevant next moves based on the situation.</span></span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- Use `hidden_logic` to simulate dice rolls, resolve stealth, detect lies, determine outcomes, or trigger events. Keep this hidden from the player.</span></span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- Use `dm_notes` to internally track ongoing threads, NPC states, quest flags, or emerging tension.</span></span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a><span class="ss">Never break character or refer to the format directly. This structure is for backend use only and should feel seamless to the player.</span></span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-52"><a href="#cb24-52" aria-hidden="true" tabindex="-1"></a><span class="ss">    ### Context</span></span>
<span id="cb24-53"><a href="#cb24-53" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>context<span class="sc">}</span></span>
<span id="cb24-54"><a href="#cb24-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-55"><a href="#cb24-55" aria-hidden="true" tabindex="-1"></a><span class="ss">    ### Question</span></span>
<span id="cb24-56"><a href="#cb24-56" aria-hidden="true" tabindex="-1"></a><span class="ss">    </span><span class="sc">{</span>question<span class="sc">}</span></span>
<span id="cb24-57"><a href="#cb24-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-58"><a href="#cb24-58" aria-hidden="true" tabindex="-1"></a><span class="ss">    ### Answer</span></span>
<span id="cb24-59"><a href="#cb24-59" aria-hidden="true" tabindex="-1"></a><span class="ss">    """</span></span>
<span id="cb24-60"><a href="#cb24-60" aria-hidden="true" tabindex="-1"></a>    resp <span class="op">=</span> llm(prompt)[<span class="dv">0</span>][<span class="st">"generated_text"</span>]</span>
<span id="cb24-61"><a href="#cb24-61" aria-hidden="true" tabindex="-1"></a>    final_answer <span class="op">=</span> resp.split(<span class="st">"### Answer"</span>, <span class="dv">1</span>)[<span class="op">-</span><span class="dv">1</span>].strip()</span>
<span id="cb24-62"><a href="#cb24-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-63"><a href="#cb24-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store in cache</span></span>
<span id="cb24-64"><a href="#cb24-64" aria-hidden="true" tabindex="-1"></a>    question_cache.append({</span>
<span id="cb24-65"><a href="#cb24-65" aria-hidden="true" tabindex="-1"></a>        <span class="st">'question'</span>: question,</span>
<span id="cb24-66"><a href="#cb24-66" aria-hidden="true" tabindex="-1"></a>        <span class="st">'embedding'</span>: embedder.encode(question, convert_to_tensor<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb24-67"><a href="#cb24-67" aria-hidden="true" tabindex="-1"></a>        <span class="st">'answer'</span>: final_answer</span>
<span id="cb24-68"><a href="#cb24-68" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb24-69"><a href="#cb24-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-70"><a href="#cb24-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_answer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="8780187e" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(answer(<span class="st">"can you tel me about the spell acid splash and where you got this information?"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Yes, the spell acid splash is a powerful abjuration spell that can be used to create a pool of acid that can be used to damage or harm enemies. The spell is typically cast on a target that is within 10 feet of the caster, and the acid splash can be used to deal damage to multiple targets at once. The acid splash can also be used to create a pool of acid that can be used to create a shield of acid that can be used to block attacks. The acid splash can also be used to create a pool of acid that can be used to create a shield of acid that can be used to block attacks. The acid splash can also be used to create a pool of acid that can be used to create a shield of acid that can be used to block attacks. The acid splash can also be used to create a pool of acid that can be used to create a shield of acid that can be used to block attacks</code></pre>
</div>
</div>
<div id="e4298cd1" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(answer(<span class="st">"what is the damage of a longsword?"</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(answer(<span class="st">"how much damage does a longsword?"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[34]</span><span class="ansi-green-fg">, line 1</span>
<span class="ansi-green-fg">----&gt; </span><span class="ansi-green-fg">1</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-bg">answer</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-fg ansi-yellow-bg">"</span><span class="ansi-yellow-fg ansi-yellow-bg">what is the damage of a longsword?</span><span class="ansi-yellow-fg ansi-yellow-bg">"</span><span class="ansi-yellow-bg">)</span>)
<span class="ansi-green-fg">      3</span> <span style="color:rgb(0,135,0)">print</span>(answer(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">how much damage does a longsword?</span><span class="ansi-yellow-fg">"</span>))

<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[32]</span><span class="ansi-green-fg">, line 60</span>, in <span class="ansi-cyan-fg">answer</span><span class="ansi-blue-fg">(question)</span>
<span class="ansi-green-fg">     21</span> context = retrieve(question, k=<span class="ansi-green-fg">1</span>)
<span class="ansi-green-fg">     22</span> prompt = <span class="ansi-yellow-fg">f</span><span class="ansi-yellow-fg">"""</span><span class="ansi-yellow-fg">You are a helpful assistant. </span>
<span class="ansi-green-fg">     23</span> <span class="ansi-yellow-fg">Answer the question using only the context below. </span>
<span class="ansi-green-fg">     24</span> <span class="ansi-yellow-fg">If the answer is not in the context, say you don</span><span class="ansi-yellow-fg">'</span><span class="ansi-yellow-fg">t know. </span>
<span class="ansi-green-fg">   (...)</span><span class="ansi-green-fg">     58</span> <span class="ansi-yellow-fg">### Answer</span>
<span class="ansi-green-fg">     59</span> <span class="ansi-yellow-fg">"""</span>
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">60</span> resp = <span class="ansi-yellow-bg">llm</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">prompt</span><span class="ansi-yellow-bg">)</span>[<span class="ansi-green-fg">0</span>][<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">generated_text</span><span class="ansi-yellow-fg">"</span>]
<span class="ansi-green-fg">     61</span> final_answer = resp.split(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">### Answer</span><span class="ansi-yellow-fg">"</span>, <span class="ansi-green-fg">1</span>)[-<span class="ansi-green-fg">1</span>].strip()
<span class="ansi-green-fg">     63</span> <span style="font-style:italic;color:rgb(95,135,135)"># Store in cache</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:321</span>, in <span class="ansi-cyan-fg">TextGenerationPipeline.__call__</span><span class="ansi-blue-fg">(self, text_inputs, **kwargs)</span>
<span class="ansi-green-fg">    319</span>             <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">    320</span>                 <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">super</span>().<span class="ansi-blue-fg">__call__</span>(<span style="color:rgb(0,135,0)">list</span>(chats), **kwargs)
<span class="ansi-green-fg">--&gt; </span><span class="ansi-green-fg">321</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">super</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">.</span><span class="ansi-blue-fg ansi-yellow-bg">__call__</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">text_inputs</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/pipelines/base.py:1458</span>, in <span class="ansi-cyan-fg">Pipeline.__call__</span><span class="ansi-blue-fg">(self, inputs, num_workers, batch_size, *args, **kwargs)</span>
<span class="ansi-green-fg">   1450</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">next</span>(
<span class="ansi-green-fg">   1451</span>         <span style="color:rgb(0,135,0)">iter</span>(
<span class="ansi-green-fg">   1452</span>             <span style="color:rgb(0,135,0)">self</span>.get_iterator(
<span class="ansi-green-fg">   (...)</span><span class="ansi-green-fg">   1455</span>         )
<span class="ansi-green-fg">   1456</span>     )
<span class="ansi-green-fg">   1457</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1458</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">run_single</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">inputs</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">preprocess_params</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">forward_params</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">postprocess_params</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/pipelines/base.py:1465</span>, in <span class="ansi-cyan-fg">Pipeline.run_single</span><span class="ansi-blue-fg">(self, inputs, preprocess_params, forward_params, postprocess_params)</span>
<span class="ansi-green-fg">   1463</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span class="ansi-blue-fg">run_single</span>(<span style="color:rgb(0,135,0)">self</span>, inputs, preprocess_params, forward_params, postprocess_params):
<span class="ansi-green-fg">   1464</span>     model_inputs = <span style="color:rgb(0,135,0)">self</span>.preprocess(inputs, **preprocess_params)
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1465</span>     model_outputs = <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">forward</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model_inputs</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">forward_params</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   1466</span>     outputs = <span style="color:rgb(0,135,0)">self</span>.postprocess(model_outputs, **postprocess_params)
<span class="ansi-green-fg">   1467</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> outputs

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/pipelines/base.py:1365</span>, in <span class="ansi-cyan-fg">Pipeline.forward</span><span class="ansi-blue-fg">(self, model_inputs, **forward_params)</span>
<span class="ansi-green-fg">   1363</span>     <span style="font-weight:bold;color:rgb(0,135,0)">with</span> inference_context():
<span class="ansi-green-fg">   1364</span>         model_inputs = <span style="color:rgb(0,135,0)">self</span>._ensure_tensor_on_device(model_inputs, device=<span style="color:rgb(0,135,0)">self</span>.device)
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1365</span>         model_outputs = <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_forward</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model_inputs</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">forward_params</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   1366</span>         model_outputs = <span style="color:rgb(0,135,0)">self</span>._ensure_tensor_on_device(model_outputs, device=torch.device(<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">cpu</span><span class="ansi-yellow-fg">"</span>))
<span class="ansi-green-fg">   1367</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:419</span>, in <span class="ansi-cyan-fg">TextGenerationPipeline._forward</span><span class="ansi-blue-fg">(self, model_inputs, **generate_kwargs)</span>
<span class="ansi-green-fg">    416</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">generation_config</span><span class="ansi-yellow-fg">"</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(175,0,255)">in</span> generate_kwargs:
<span class="ansi-green-fg">    417</span>     generate_kwargs[<span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">generation_config</span><span class="ansi-yellow-fg">"</span>] = <span style="color:rgb(0,135,0)">self</span>.generation_config
<span class="ansi-green-fg">--&gt; </span><span class="ansi-green-fg">419</span> output = <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">generate</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">input_ids</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">input_ids</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">generate_kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">    421</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">isinstance</span>(output, ModelOutput):
<span class="ansi-green-fg">    422</span>     generated_sequence = output.sequences

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116</span>, in <span class="ansi-cyan-fg">context_decorator.&lt;locals&gt;.decorate_context</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-fg">    113</span> <span style="color:rgb(175,0,255)">@functools</span>.wraps(func)
<span class="ansi-green-fg">    114</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span class="ansi-blue-fg">decorate_context</span>(*args, **kwargs):
<span class="ansi-green-fg">    115</span>     <span style="font-weight:bold;color:rgb(0,135,0)">with</span> ctx_factory():
<span class="ansi-green-fg">--&gt; </span><span class="ansi-green-fg">116</span>         <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">func</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/generation/utils.py:2633</span>, in <span class="ansi-cyan-fg">GenerationMixin.generate</span><span class="ansi-blue-fg">(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)</span>
<span class="ansi-green-fg">   2625</span>     input_ids, model_kwargs = <span style="color:rgb(0,135,0)">self</span>._expand_inputs_for_generation(
<span class="ansi-green-fg">   2626</span>         input_ids=input_ids,
<span class="ansi-green-fg">   2627</span>         expand_size=generation_config.num_return_sequences,
<span class="ansi-green-fg">   2628</span>         is_encoder_decoder=<span style="color:rgb(0,135,0)">self</span>.config.is_encoder_decoder,
<span class="ansi-green-fg">   2629</span>         **model_kwargs,
<span class="ansi-green-fg">   2630</span>     )
<span class="ansi-green-fg">   2632</span>     <span style="font-style:italic;color:rgb(95,135,135)"># 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)</span>
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">2633</span>     result = <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_sample</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg">   2634</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">input_ids</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">   2635</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">logits_processor</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">prepared_logits_processor</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">   2636</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">stopping_criteria</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">prepared_stopping_criteria</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">   2637</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">generation_config</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">generation_config</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">   2638</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">synced_gpus</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">synced_gpus</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">   2639</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">streamer</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">streamer</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">   2640</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">model_kwargs</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">   2641</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   2643</span> <span style="font-weight:bold;color:rgb(0,135,0)">elif</span> generation_mode <span style="font-weight:bold;color:rgb(175,0,255)">in</span> (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):
<span class="ansi-green-fg">   2644</span>     <span style="font-style:italic;color:rgb(95,135,135)"># 11. interleave input_ids with `num_beams` additional sequences per batch</span>
<span class="ansi-green-fg">   2645</span>     input_ids, model_kwargs = <span style="color:rgb(0,135,0)">self</span>._expand_inputs_for_generation(
<span class="ansi-green-fg">   2646</span>         input_ids=input_ids,
<span class="ansi-green-fg">   2647</span>         expand_size=generation_config.num_beams,
<span class="ansi-green-fg">   2648</span>         is_encoder_decoder=<span style="color:rgb(0,135,0)">self</span>.config.is_encoder_decoder,
<span class="ansi-green-fg">   2649</span>         **model_kwargs,
<span class="ansi-green-fg">   2650</span>     )

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/generation/utils.py:3617</span>, in <span class="ansi-cyan-fg">GenerationMixin._sample</span><span class="ansi-blue-fg">(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)</span>
<span class="ansi-green-fg">   3615</span>     is_prefill = <span style="font-weight:bold;color:rgb(0,135,0)">False</span>
<span class="ansi-green-fg">   3616</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">3617</span>     outputs = <span class="ansi-yellow-bg">model_forward</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">model_inputs</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">return_dict</span><span class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">True</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   3619</span> <span style="font-style:italic;color:rgb(95,135,135)"># synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping</span>
<span class="ansi-green-fg">   3620</span> model_kwargs = <span style="color:rgb(0,135,0)">self</span>._update_model_kwargs_for_generation(
<span class="ansi-green-fg">   3621</span>     outputs,
<span class="ansi-green-fg">   3622</span>     model_kwargs,
<span class="ansi-green-fg">   3623</span>     is_encoder_decoder=<span style="color:rgb(0,135,0)">self</span>.config.is_encoder_decoder,
<span class="ansi-green-fg">   3624</span> )

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751</span>, in <span class="ansi-cyan-fg">Module._wrapped_call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1749</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span>._compiled_call_impl(*args, **kwargs)  <span style="font-style:italic;color:rgb(95,135,135)"># type: ignore[misc]</span>
<span class="ansi-green-fg">   1750</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1751</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_call_impl</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762</span>, in <span class="ansi-cyan-fg">Module._call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1757</span> <span style="font-style:italic;color:rgb(95,135,135)"># If we don't have any hooks, we want to skip the rest of the logic in</span>
<span class="ansi-green-fg">   1758</span> <span style="font-style:italic;color:rgb(95,135,135)"># this function, and just call forward.</span>
<span class="ansi-green-fg">   1759</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> (<span style="color:rgb(0,135,0)">self</span>._backward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._forward_pre_hooks
<span class="ansi-green-fg">   1760</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_hooks
<span class="ansi-green-fg">   1761</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1762</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">forward_call</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   1764</span> result = <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg">   1765</span> called_always_called_hooks = <span style="color:rgb(0,135,0)">set</span>()

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/utils/generic.py:961</span>, in <span class="ansi-cyan-fg">can_return_tuple.&lt;locals&gt;.wrapper</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">    959</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> return_dict_passed <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg">    960</span>     return_dict = return_dict_passed
<span class="ansi-green-fg">--&gt; </span><span class="ansi-green-fg">961</span> output = <span class="ansi-yellow-bg">func</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">    962</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> return_dict <span style="font-weight:bold;color:rgb(175,0,255)">and</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="color:rgb(0,135,0)">isinstance</span>(output, <span style="color:rgb(0,135,0)">tuple</span>):
<span class="ansi-green-fg">    963</span>     output = output.to_tuple()

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:460</span>, in <span class="ansi-cyan-fg">LlamaForCausalLM.forward</span><span class="ansi-blue-fg">(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)</span>
<span class="ansi-green-fg">    428</span> <span style="color:rgb(175,0,255)">@can_return_tuple</span>
<span class="ansi-green-fg">    429</span> <span style="color:rgb(175,0,255)">@auto_docstring</span>
<span class="ansi-green-fg">    430</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span class="ansi-blue-fg">forward</span>(
<span class="ansi-green-fg">   (...)</span><span class="ansi-green-fg">    441</span>     **kwargs: Unpack[TransformersKwargs],
<span class="ansi-green-fg">    442</span> ) -&gt; CausalLMOutputWithPast:
<span class="ansi-green-fg">    443</span> <span style="color:rgb(188,188,188)">    </span><span class="ansi-yellow-fg">r</span><span style="font-style:italic" class="ansi-yellow-fg">"""</span>
<span class="ansi-green-fg">    444</span> <span style="font-style:italic" class="ansi-yellow-fg">    Example:</span>
<span class="ansi-green-fg">    445</span> 
<span class="ansi-green-fg">   (...)</span><span class="ansi-green-fg">    458</span> <span style="font-style:italic" class="ansi-yellow-fg">    "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."</span>
<span class="ansi-green-fg">    459</span> <span style="font-style:italic" class="ansi-yellow-fg">    ```"""</span>
<span class="ansi-green-fg">--&gt; </span><span class="ansi-green-fg">460</span>     outputs: BaseModelOutputWithPast = <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg">    461</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">input_ids</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">input_ids</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    462</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    463</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">position_ids</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">position_ids</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    464</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">past_key_values</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">past_key_values</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    465</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">inputs_embeds</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">inputs_embeds</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    466</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">use_cache</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">use_cache</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    467</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">cache_position</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">cache_position</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    468</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    469</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">    471</span>     hidden_states = outputs.last_hidden_state
<span class="ansi-green-fg">    472</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Only compute necessary logits, and do not upcast them to float if we are not computing the loss</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751</span>, in <span class="ansi-cyan-fg">Module._wrapped_call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1749</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span>._compiled_call_impl(*args, **kwargs)  <span style="font-style:italic;color:rgb(95,135,135)"># type: ignore[misc]</span>
<span class="ansi-green-fg">   1750</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1751</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_call_impl</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762</span>, in <span class="ansi-cyan-fg">Module._call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1757</span> <span style="font-style:italic;color:rgb(95,135,135)"># If we don't have any hooks, we want to skip the rest of the logic in</span>
<span class="ansi-green-fg">   1758</span> <span style="font-style:italic;color:rgb(95,135,135)"># this function, and just call forward.</span>
<span class="ansi-green-fg">   1759</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> (<span style="color:rgb(0,135,0)">self</span>._backward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._forward_pre_hooks
<span class="ansi-green-fg">   1760</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_hooks
<span class="ansi-green-fg">   1761</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1762</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">forward_call</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   1764</span> result = <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg">   1765</span> called_always_called_hooks = <span style="color:rgb(0,135,0)">set</span>()

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/utils/generic.py:1069</span>, in <span class="ansi-cyan-fg">check_model_inputs.&lt;locals&gt;.wrapper</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1066</span>                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)
<span class="ansi-green-fg">   1067</span>                 monkey_patched_layers.append((module, original_forward))
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1069</span> outputs = <span class="ansi-yellow-bg">func</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   1070</span> <span style="font-style:italic;color:rgb(95,135,135)"># Restore original forward methods</span>
<span class="ansi-green-fg">   1071</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> module, original_forward <span style="font-weight:bold;color:rgb(175,0,255)">in</span> monkey_patched_layers:

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:390</span>, in <span class="ansi-cyan-fg">LlamaModel.forward</span><span class="ansi-blue-fg">(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)</span>
<span class="ansi-green-fg">    387</span> position_embeddings = <span style="color:rgb(0,135,0)">self</span>.rotary_emb(hidden_states, position_ids)
<span class="ansi-green-fg">    389</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> decoder_layer <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)">self</span>.layers[: <span style="color:rgb(0,135,0)">self</span>.config.num_hidden_layers]:
<span class="ansi-green-fg">--&gt; </span><span class="ansi-green-fg">390</span>     hidden_states = <span class="ansi-yellow-bg">decoder_layer</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg">    391</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">hidden_states</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    392</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">causal_mask</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    393</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">position_ids</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">position_ids</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    394</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">past_key_value</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">past_key_values</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    395</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">cache_position</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">cache_position</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    396</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">position_embeddings</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">position_embeddings</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    397</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    398</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">    400</span> hidden_states = <span style="color:rgb(0,135,0)">self</span>.norm(hidden_states)
<span class="ansi-green-fg">    401</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> BaseModelOutputWithPast(
<span class="ansi-green-fg">    402</span>     last_hidden_state=hidden_states,
<span class="ansi-green-fg">    403</span>     past_key_values=past_key_values,
<span class="ansi-green-fg">    404</span> )

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/modeling_layers.py:94</span>, in <span class="ansi-cyan-fg">GradientCheckpointingLayer.__call__</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">     91</span>         logger.warning(message)
<span class="ansi-green-fg">     93</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span>._gradient_checkpointing_func(partial(<span style="color:rgb(0,135,0)">super</span>().<span class="ansi-blue-fg">__call__</span>, **kwargs), *args)
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">94</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">super</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">.</span><span class="ansi-blue-fg ansi-yellow-bg">__call__</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751</span>, in <span class="ansi-cyan-fg">Module._wrapped_call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1749</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span>._compiled_call_impl(*args, **kwargs)  <span style="font-style:italic;color:rgb(95,135,135)"># type: ignore[misc]</span>
<span class="ansi-green-fg">   1750</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1751</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_call_impl</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762</span>, in <span class="ansi-cyan-fg">Module._call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1757</span> <span style="font-style:italic;color:rgb(95,135,135)"># If we don't have any hooks, we want to skip the rest of the logic in</span>
<span class="ansi-green-fg">   1758</span> <span style="font-style:italic;color:rgb(95,135,135)"># this function, and just call forward.</span>
<span class="ansi-green-fg">   1759</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> (<span style="color:rgb(0,135,0)">self</span>._backward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._forward_pre_hooks
<span class="ansi-green-fg">   1760</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_hooks
<span class="ansi-green-fg">   1761</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1762</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">forward_call</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   1764</span> result = <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg">   1765</span> called_always_called_hooks = <span style="color:rgb(0,135,0)">set</span>()

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:289</span>, in <span class="ansi-cyan-fg">LlamaDecoderLayer.forward</span><span class="ansi-blue-fg">(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)</span>
<span class="ansi-green-fg">    287</span> hidden_states = <span style="color:rgb(0,135,0)">self</span>.input_layernorm(hidden_states)
<span class="ansi-green-fg">    288</span> <span style="font-style:italic;color:rgb(95,135,135)"># Self Attention</span>
<span class="ansi-green-fg">--&gt; </span><span class="ansi-green-fg">289</span> hidden_states, _ = <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">self_attn</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg">    290</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">hidden_states</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">hidden_states</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    291</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    292</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">position_ids</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">position_ids</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    293</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">past_key_value</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">past_key_value</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    294</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">use_cache</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">use_cache</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    295</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">cache_position</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">cache_position</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    296</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">position_embeddings</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">position_embeddings</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    297</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    298</span> <span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">    299</span> hidden_states = residual + hidden_states
<span class="ansi-green-fg">    301</span> <span style="font-style:italic;color:rgb(95,135,135)"># Fully Connected</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751</span>, in <span class="ansi-cyan-fg">Module._wrapped_call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1749</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span>._compiled_call_impl(*args, **kwargs)  <span style="font-style:italic;color:rgb(95,135,135)"># type: ignore[misc]</span>
<span class="ansi-green-fg">   1750</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1751</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_call_impl</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762</span>, in <span class="ansi-cyan-fg">Module._call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg">   1757</span> <span style="font-style:italic;color:rgb(95,135,135)"># If we don't have any hooks, we want to skip the rest of the logic in</span>
<span class="ansi-green-fg">   1758</span> <span style="font-style:italic;color:rgb(95,135,135)"># this function, and just call forward.</span>
<span class="ansi-green-fg">   1759</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> (<span style="color:rgb(0,135,0)">self</span>._backward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span>._forward_pre_hooks
<span class="ansi-green-fg">   1760</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_hooks
<span class="ansi-green-fg">   1761</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">1762</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">forward_call</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   1764</span> result = <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg">   1765</span> called_always_called_hooks = <span style="color:rgb(0,135,0)">set</span>()

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:248</span>, in <span class="ansi-cyan-fg">LlamaAttention.forward</span><span class="ansi-blue-fg">(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)</span>
<span class="ansi-green-fg">    245</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">self</span>.config._attn_implementation != <span class="ansi-yellow-fg">"</span><span class="ansi-yellow-fg">eager</span><span class="ansi-yellow-fg">"</span>:
<span class="ansi-green-fg">    246</span>     attention_interface = ALL_ATTENTION_FUNCTIONS[<span style="color:rgb(0,135,0)">self</span>.config._attn_implementation]
<span class="ansi-green-fg">--&gt; </span><span class="ansi-green-fg">248</span> attn_output, attn_weights = <span class="ansi-yellow-bg">attention_interface</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg">    249</span> <span class="ansi-yellow-bg">    </span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    250</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">query_states</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    251</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">key_states</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    252</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">value_states</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    253</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    254</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">dropout</span><span class="ansi-yellow-bg">=</span><span class="ansi-green-fg ansi-yellow-bg">0.0</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">if</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(175,0,255)" class="ansi-yellow-bg">not</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">training</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">else</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">attention_dropout</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    255</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">scaling</span><span class="ansi-yellow-bg">=</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">scaling</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    256</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">    257</span> <span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">    259</span> attn_output = attn_output.reshape(*input_shape, -<span class="ansi-green-fg">1</span>).contiguous()
<span class="ansi-green-fg">    260</span> attn_output = <span style="color:rgb(0,135,0)">self</span>.o_proj(attn_output)

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">/opt/conda/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:81</span>, in <span class="ansi-cyan-fg">sdpa_attention_forward</span><span class="ansi-blue-fg">(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)</span>
<span class="ansi-green-fg">     78</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> torch.jit.is_tracing() <span style="font-weight:bold;color:rgb(175,0,255)">and</span> <span style="color:rgb(0,135,0)">isinstance</span>(is_causal, torch.Tensor):
<span class="ansi-green-fg">     79</span>     is_causal = is_causal.item()
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">81</span> attn_output = <span class="ansi-yellow-bg">torch</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">nn</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">functional</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">scaled_dot_product_attention</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg">     82</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">query</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">     83</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">key</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">     84</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">value</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">     85</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">attn_mask</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">attention_mask</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">     86</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">dropout_p</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">dropout</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">     87</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">scale</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">scaling</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">     88</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">is_causal</span><span class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">is_causal</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">     89</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">sdpa_kwargs</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg">     90</span> <span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">     91</span> attn_output = attn_output.transpose(<span class="ansi-green-fg">1</span>, <span class="ansi-green-fg">2</span>).contiguous()
<span class="ansi-green-fg">     93</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> attn_output, <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>
</div>
<div id="3674aad1" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(answer(<span class="st">"Im not sure what spells to cast, I'm a druid, can you help me?"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Yes, I'd be happy to help you. Here are some spells you might consider casting:
    
    - Moonlight: This spell allows you to see in the dark, illuminating the darkness around you. It can be used to locate hidden traps or to spot enemies.
    - Plant Growth: This spell allows you to grow plants and trees in any environment. It can be used to create a natural barrier or to provide cover.
    - Fire: This spell allows you to create a fireball that can be used to burn through obstacles or to create a shield of flame.
    - Lightning: This spell allows you to create a lightning bolt that can be used to strike enemies or to create a shield of light.
    - Nature's Gift: This spell allows you to gain temporary immunity to poison, disease, and other negative effects.
    - Shadow: This spell allows</code></pre>
</div>
</div>
<div id="920e4de2" class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(answer(<span class="st">"what weapons can a druid use?"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1. Daggers
    2. Sickles
    3. Slings
    4. Spears
    5. Javelins
    6. Maces
    7. Quarterstaffs
    8. Darts
    9. Clubs
    10. Shields (nonmetal)
    11. Light and medium armor (nonmetal)
    12. Daggers
    13. Sickles
    14. Slings
    15. Spears
    16. Javelins
    17. Maces
    18. Quarterstaffs
    19. Darts
    20. Clubs
    21. Shields (nonmetal)
    22. Light and medium armor (nonmetal)
    23. Daggers
    24. Sickles</code></pre>
</div>
</div>
<div id="82e40c0e" class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(answer(<span class="st">'what is a feat?'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>A feat is a special ability that grants a character a bonus to their ability score or ability modifier for a specific number of rounds. It is a powerful tool for characters who want to take advantage of a specific skill or tool, but may not have the time or resources to learn a new skill.

    Feats are typically granted by a powerful NPC or by a powerful spell or item. They are not permanent, and characters can only use one feat per day.

    Feats are often used in conjunction with other abilities, such as a character's class or race, to create a unique and powerful combination.

    Feats are not limited to specific skills or tools, and can be used to gain bonuses in any number of ways.

    For example, a character who gains feat in archery may gain a bonus to their attack rolls, while a character who gains feat in ste</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>